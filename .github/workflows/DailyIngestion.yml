name: JavDB Daily Ingestion Pipeline

permissions:
  contents: read

on:
  workflow_dispatch:
  # schedule:
  #   # Run daily at 10:00 UTC (18:00 Beijing Time)
  #   - cron: '0 10 * * *'

jobs:
  # =============================================================================
  # Job 1: Setup - Checkout, install dependencies, generate config
  # =============================================================================
  setup:
    runs-on: ubuntu-latest
    environment: Production
    permissions:
      contents: read
    outputs:
      branch: ${{ steps.branch_check.outputs.branch }}
    env:
      TZ: Asia/Singapore
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ssh-key: ${{ secrets.DEPLOY_KEY }}

      - name: Get current branch
        id: branch_check
        run: |
          CURRENT_BRANCH="${GITHUB_REF#refs/heads/}"
          echo "Current branch: $CURRENT_BRANCH"
          echo "branch=$CURRENT_BRANCH" >> $GITHUB_OUTPUT

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Generate config.py from GitHub Variables and Secrets
        env:
          # ============ SECRETS (sensitive: passwords, tokens, IPs) ============
          VAR_QB_HOST: ${{ secrets.QB_HOST }}
          VAR_QB_PASSWORD: ${{ secrets.QB_PASSWORD }}
          VAR_SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD }}
          VAR_PROXY_POOL_JSON: ${{ secrets.PROXY_POOL_JSON }}
          VAR_JAVDB_PASSWORD: ${{ secrets.JAVDB_PASSWORD }}
          VAR_JAVDB_SESSION_COOKIE: ${{ secrets.JAVDB_SESSION_COOKIE }}
          VAR_PIKPAK_PASSWORD: ${{ secrets.PIKPAK_PASSWORD }}
          # ============ VARIABLES (non-sensitive) ============
          VAR_GIT_REPO_URL: ${{ vars.GIT_REPO_URL }}
          VAR_GIT_BRANCH: ${{ vars.GIT_BRANCH }}
          VAR_QB_PORT: ${{ vars.QB_PORT }}
          VAR_QB_USERNAME: ${{ secrets.QB_USERNAME }}
          VAR_TORRENT_CATEGORY: ${{ vars.TORRENT_CATEGORY }}
          VAR_TORRENT_CATEGORY_ADHOC: ${{ vars.TORRENT_CATEGORY_ADHOC }}
          VAR_TORRENT_SAVE_PATH: ${{ vars.TORRENT_SAVE_PATH }}
          VAR_AUTO_START: ${{ vars.AUTO_START }}
          VAR_SKIP_CHECKING: ${{ vars.SKIP_CHECKING || 'False' }}
          VAR_REQUEST_TIMEOUT: ${{ vars.REQUEST_TIMEOUT || '30' }}
          VAR_DELAY_BETWEEN_ADDITIONS: ${{ vars.DELAY_BETWEEN_ADDITIONS }}
          VAR_SMTP_SERVER: ${{ secrets.SMTP_SERVER }}
          VAR_SMTP_PORT: ${{ vars.SMTP_PORT }}
          VAR_SMTP_USER: ${{ secrets.SMTP_USER }}
          VAR_EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
          VAR_EMAIL_TO: ${{ secrets.EMAIL_TO }}
          VAR_PROXY_MODE: ${{ vars.PROXY_MODE }}
          VAR_PROXY_POOL_COOLDOWN_SECONDS: ${{ vars.PROXY_POOL_COOLDOWN_SECONDS || '691200' }}
          VAR_PROXY_POOL_MAX_FAILURES: ${{ vars.PROXY_POOL_MAX_FAILURES || 3 }}
          VAR_PROXY_MODULES_JSON: ${{ vars.PROXY_MODULES_JSON }}
          VAR_CF_BYPASS_SERVICE_PORT: ${{ vars.CF_BYPASS_SERVICE_PORT }}
          VAR_CF_BYPASS_ENABLED: ${{ vars.CF_BYPASS_ENABLED || 'True' }}
          VAR_START_PAGE: ${{ vars.START_PAGE }}
          VAR_END_PAGE: ${{ vars.END_PAGE }}
          VAR_PHASE2_MIN_RATE: ${{ vars.PHASE2_MIN_RATE }}
          VAR_PHASE2_MIN_COMMENTS: ${{ vars.PHASE2_MIN_COMMENTS }}
          VAR_BASE_URL: ${{ vars.BASE_URL || 'https://javdb.com' }}
          VAR_JAVDB_USERNAME: ${{ secrets.JAVDB_USERNAME }}
          VAR_DETAIL_PAGE_SLEEP: ${{ vars.DETAIL_PAGE_SLEEP }}
          VAR_PAGE_SLEEP: ${{ vars.PAGE_SLEEP }}
          VAR_MOVIE_SLEEP: ${{ vars.MOVIE_SLEEP }}
          VAR_CF_TURNSTILE_COOLDOWN: ${{ vars.CF_TURNSTILE_COOLDOWN }}
          VAR_PHASE_TRANSITION_COOLDOWN: ${{ vars.PHASE_TRANSITION_COOLDOWN }}
          VAR_FALLBACK_COOLDOWN: ${{ vars.FALLBACK_COOLDOWN }}
          VAR_LOG_LEVEL: ${{ vars.LOG_LEVEL || 'INFO' }}
          VAR_SPIDER_LOG_FILE: ${{ vars.SPIDER_LOG_FILE || 'logs/spider.log' }}
          VAR_UPLOADER_LOG_FILE: ${{ vars.UPLOADER_LOG_FILE || 'logs/qb_uploader.log' }}
          VAR_PIPELINE_LOG_FILE: ${{ vars.PIPELINE_LOG_FILE || 'logs/pipeline.log' }}
          VAR_EMAIL_NOTIFICATION_LOG_FILE: ${{ vars.EMAIL_NOTIFICATION_LOG_FILE || 'logs/email_notification.log' }}
          VAR_IGNORE_RELEASE_DATE_FILTER: ${{ vars.IGNORE_RELEASE_DATE_FILTER || 'False' }}
          VAR_REPORTS_DIR: ${{ vars.REPORTS_DIR || 'reports' }}
          VAR_DAILY_REPORT_DIR: ${{ vars.DAILY_REPORT_DIR || 'reports/DailyReport' }}
          VAR_AD_HOC_DIR: ${{ vars.AD_HOC_DIR || 'reports/AdHoc' }}
          VAR_PARSED_MOVIES_CSV: ${{ vars.PARSED_MOVIES_CSV }}
          VAR_PIKPAK_EMAIL: ${{ secrets.PIKPAK_EMAIL }}
          VAR_PIKPAK_LOG_FILE: ${{ vars.PIKPAK_LOG_FILE || 'logs/pikpak_bridge.log' }}
          VAR_PIKPAK_REQUEST_DELAY: ${{ vars.PIKPAK_REQUEST_DELAY }}
        run: python3 utils/config_generator.py --github-actions

      - name: Pull latest changes from remote
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          CURRENT_BRANCH="${{ steps.branch_check.outputs.branch }}"
          echo "Pulling latest changes from branch: $CURRENT_BRANCH"
          
          REPO_URL=$(git remote get-url origin)
          if [[ "$REPO_URL" == https://* ]]; then
            REPO_SSH=$(echo "$REPO_URL" | sed 's|https://github.com/|git@github.com:|')
            git remote set-url origin "$REPO_SSH"
            echo "Updated remote URL to SSH: $REPO_SSH"
          fi
          
          git fetch origin "$CURRENT_BRANCH"
          git pull --rebase origin "$CURRENT_BRANCH" || {
            echo "Warning: Failed to pull/rebase, attempting merge instead"
            git pull --no-edit origin "$CURRENT_BRANCH" || {
              echo "Error: Could not integrate remote changes"
              exit 1
            }
          }

      - name: Encrypt config for artifact
        env:
          ARTIFACT_KEY: ${{ secrets.ARTIFACT_KEY }}
        run: |
          # Encrypt config.py using AES-256-CBC before uploading
          # All artifacts are encrypted to protect sensitive data
          openssl enc -aes-256-cbc -salt -pbkdf2 -iter 100000 \
            -in config.py -out config.py.enc \
            -pass pass:"$ARTIFACT_KEY"
          echo "✓ Config encrypted successfully"

      - name: Upload encrypted config artifact
        uses: actions/upload-artifact@v4
        with:
          name: config-encrypted
          path: config.py.enc
          retention-days: 1

  # =============================================================================
  # Job 2: Health Check - Verify essential services before running pipeline
  # =============================================================================
  health-check:
    runs-on: self-hosted
    environment: Production
    permissions:
      contents: read
    needs: setup
    env:
      TZ: Asia/Singapore
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download encrypted config
        uses: actions/download-artifact@v4
        with:
          name: config-encrypted
          path: .

      - name: Decrypt config
        env:
          ARTIFACT_KEY: ${{ secrets.ARTIFACT_KEY }}
        run: |
          openssl enc -aes-256-cbc -d -pbkdf2 -iter 100000 \
            -in config.py.enc -out config.py \
            -pass pass:"$ARTIFACT_KEY"
          rm config.py.enc
          echo "✓ Config decrypted successfully"

      - name: Run health checks
        run: python3 scripts/health_check.py --use-proxy

  # =============================================================================
  # Job 3: Run Pipeline - Execute spider, uploader, and pikpak bridge
  # =============================================================================
  run-pipeline:
    runs-on: self-hosted
    environment: Production
    permissions:
      contents: read
    needs: [setup, health-check]
    outputs:
      spider_success: ${{ steps.spider.outcome == 'success' }}
      uploader_success: ${{ steps.uploader.outcome == 'success' }}
      pikpak_success: ${{ steps.pikpak.outcome == 'success' }}
      csv_filename: ${{ steps.spider.outputs.csv_filename }}
    env:
      TZ: Asia/Singapore
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ssh-key: ${{ secrets.DEPLOY_KEY }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download encrypted config
        uses: actions/download-artifact@v4
        with:
          name: config-encrypted
          path: .

      - name: Decrypt config
        env:
          ARTIFACT_KEY: ${{ secrets.ARTIFACT_KEY }}
        run: |
          openssl enc -aes-256-cbc -d -pbkdf2 -iter 100000 \
            -in config.py.enc -out config.py \
            -pass pass:"$ARTIFACT_KEY"
          rm config.py.enc
          echo "✓ Config decrypted successfully"

      - name: Pull latest changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          CURRENT_BRANCH="${{ needs.setup.outputs.branch }}"
          
          REPO_URL=$(git remote get-url origin)
          if [[ "$REPO_URL" == https://* ]]; then
            REPO_SSH=$(echo "$REPO_URL" | sed 's|https://github.com/|git@github.com:|')
            git remote set-url origin "$REPO_SSH"
          fi
          
          git fetch origin "$CURRENT_BRANCH"
          git pull --rebase origin "$CURRENT_BRANCH" || git pull --no-edit origin "$CURRENT_BRANCH" || true

      - name: Step 1 - Run Spider
        id: spider
        run: |
          # Capture output to extract the CSV filename
          SPIDER_OUTPUT=$(python3 scripts/spider.py --use-proxy | tee /dev/stderr)
          
          # Extract CSV filename from spider output (format: SPIDER_OUTPUT_CSV=filename.csv)
          CSV_FILENAME=$(echo "$SPIDER_OUTPUT" | grep "^SPIDER_OUTPUT_CSV=" | cut -d'=' -f2 | tail -1)
          if [ -n "$CSV_FILENAME" ]; then
            echo "csv_filename=$CSV_FILENAME" >> $GITHUB_OUTPUT
            echo "Captured CSV filename: $CSV_FILENAME"
          else
            echo "Warning: Could not capture CSV filename from spider output"
          fi

      - name: Step 2 - Run qBittorrent Uploader
        id: uploader
        run: |
          CMD="python3 scripts/qb_uploader.py --mode daily --use-proxy"
          
          # Pass the CSV full path captured from spider
          CSV_PATH="${{ steps.spider.outputs.csv_filename }}"
          if [ -n "$CSV_PATH" ]; then
            CMD="$CMD --input-file '$CSV_PATH'"
            echo "Using CSV file from spider: $CSV_PATH"
          fi
          
          echo "Executing: $CMD"
          eval $CMD

      - name: Step 3 - Run PikPak Bridge
        id: pikpak
        if: ${{ success() && steps.uploader.outcome == 'success' }}
        run: python3 scripts/pikpak_bridge.py --days 3

      - name: Encrypt and upload logs artifact
        if: always()
        env:
          ARTIFACT_KEY: ${{ secrets.ARTIFACT_KEY }}
        run: |
          # Create encrypted archive of logs (including .log and .txt files)
          # .txt files include proxy ban HTML captures for debugging
          mkdir -p encrypted_artifacts
          
          # Build list of files to archive
          LOG_FILES=""
          if ls logs/*.log 1> /dev/null 2>&1; then
            LOG_FILES="logs/*.log"
          fi
          if ls logs/*.txt 1> /dev/null 2>&1; then
            LOG_FILES="$LOG_FILES logs/*.txt"
          fi
          
          if [ -n "$LOG_FILES" ]; then
            tar -czf - $LOG_FILES | openssl enc -aes-256-cbc -salt -pbkdf2 -iter 100000 \
              -out encrypted_artifacts/logs.tar.gz.enc \
              -pass pass:"$ARTIFACT_KEY"
            echo "✓ Logs encrypted successfully (including proxy ban HTML files if any)"
          else
            echo "No log files found to encrypt"
          fi

      - name: Upload encrypted logs artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-encrypted
          path: encrypted_artifacts/logs.tar.gz.enc
          retention-days: 7
          if-no-files-found: ignore

      - name: Encrypt and upload reports artifact
        if: always()
        env:
          ARTIFACT_KEY: ${{ secrets.ARTIFACT_KEY }}
          REPORTS_DIR: ${{ vars.REPORTS_DIR || 'reports' }}
        run: |
          # Create encrypted archive of all reports (preserving folder structure)
          # This includes: CSV report, history files, proxy_bans.csv
          mkdir -p encrypted_artifacts
          
          # Build list of files to archive
          FILES_TO_ARCHIVE=""
          
          # Add spider output CSV if it exists
          CSV_PATH="${{ steps.spider.outputs.csv_filename }}"
          if [ -n "$CSV_PATH" ] && [ -f "$CSV_PATH" ]; then
            FILES_TO_ARCHIVE="$CSV_PATH"
            echo "Including CSV: $CSV_PATH"
          fi
          
          # Add history files if they exist
          for HIST_FILE in "$REPORTS_DIR/parsed_movies_history.csv" "$REPORTS_DIR/pikpak_bridge_history.csv" "$REPORTS_DIR/proxy_bans.csv"; do
            if [ -f "$HIST_FILE" ]; then
              FILES_TO_ARCHIVE="$FILES_TO_ARCHIVE $HIST_FILE"
              echo "Including history: $HIST_FILE"
            fi
          done
          
          if [ -n "$FILES_TO_ARCHIVE" ]; then
            # Use tar with explicit file list to preserve folder structure
            tar -czf - $FILES_TO_ARCHIVE | openssl enc -aes-256-cbc -salt -pbkdf2 -iter 100000 \
              -out encrypted_artifacts/reports.tar.gz.enc \
              -pass pass:"$ARTIFACT_KEY"
            echo "✓ Reports encrypted successfully (folder structure preserved)"
          else
            echo "No report files found to archive"
          fi

      - name: Upload encrypted reports artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-reports-encrypted
          path: encrypted_artifacts/reports.tar.gz.enc
          retention-days: 7
          if-no-files-found: ignore

  # =============================================================================
  # Job 4: Email Notification - Always runs (success, failure, or cancel)
  # (Runs in parallel with commit-results on GitHub-hosted runner)
  # =============================================================================
  email-notification:
    runs-on: ubuntu-latest
    environment: Production
    permissions:
      contents: read
    needs: [setup, health-check, run-pipeline]
    if: always()
    env:
      TZ: Asia/Singapore
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download encrypted config
        uses: actions/download-artifact@v4
        with:
          name: config-encrypted
          path: .

      - name: Decrypt config
        env:
          ARTIFACT_KEY: ${{ secrets.ARTIFACT_KEY }}
        run: |
          openssl enc -aes-256-cbc -d -pbkdf2 -iter 100000 \
            -in config.py.enc -out config.py \
            -pass pass:"$ARTIFACT_KEY"
          rm config.py.enc
          echo "✓ Config decrypted successfully"

      - name: Download encrypted logs (if available)
        uses: actions/download-artifact@v4
        with:
          name: pipeline-logs-encrypted
          path: encrypted_artifacts
        continue-on-error: true

      - name: Decrypt logs (if available)
        env:
          ARTIFACT_KEY: ${{ secrets.ARTIFACT_KEY }}
        run: |
          if [ -f encrypted_artifacts/logs.tar.gz.enc ]; then
            openssl enc -aes-256-cbc -d -pbkdf2 -iter 100000 \
              -in encrypted_artifacts/logs.tar.gz.enc \
              -pass pass:"$ARTIFACT_KEY" | tar -xzf -
            echo "✓ Logs decrypted successfully"
          else
            echo "No encrypted logs found"
            mkdir -p logs
          fi

      - name: Download encrypted reports (if available)
        uses: actions/download-artifact@v4
        with:
          name: pipeline-reports-encrypted
          path: encrypted_artifacts
        continue-on-error: true

      - name: Decrypt reports (if available)
        env:
          ARTIFACT_KEY: ${{ secrets.ARTIFACT_KEY }}
        run: |
          if [ -f encrypted_artifacts/reports.tar.gz.enc ]; then
            openssl enc -aes-256-cbc -d -pbkdf2 -iter 100000 \
              -in encrypted_artifacts/reports.tar.gz.enc \
              -pass pass:"$ARTIFACT_KEY" | tar -xzf -
            rm -rf encrypted_artifacts
            echo "✓ Reports decrypted successfully (folder structure preserved)"
            # Show what was extracted
            CSV_PATH="${{ needs.run-pipeline.outputs.csv_filename }}"
            if [ -n "$CSV_PATH" ] && [ -f "$CSV_PATH" ]; then
              echo "CSV file restored: $CSV_PATH"
            fi
          else
            echo "No encrypted reports found"
          fi

      - name: Determine pipeline status
        id: status
        run: |
          # Check the status of all previous jobs
          SETUP_STATUS="${{ needs.setup.result }}"
          HEALTH_STATUS="${{ needs.health-check.result }}"
          PIPELINE_STATUS="${{ needs.run-pipeline.result }}"
          
          echo "Setup: $SETUP_STATUS"
          echo "Health Check: $HEALTH_STATUS"
          echo "Pipeline: $PIPELINE_STATUS"
          
          # Ensure logs directory exists
          mkdir -p logs
          
          # Create a status file for email notification to read
          echo "SETUP_STATUS=$SETUP_STATUS" >> logs/job_status.txt
          echo "HEALTH_CHECK_STATUS=$HEALTH_STATUS" >> logs/job_status.txt
          echo "PIPELINE_STATUS=$PIPELINE_STATUS" >> logs/job_status.txt
          
          if [[ "$SETUP_STATUS" == "failure" ]] || [[ "$HEALTH_STATUS" == "failure" ]] || [[ "$PIPELINE_STATUS" == "failure" ]]; then
            echo "has_failure=true" >> $GITHUB_OUTPUT
            echo "Pipeline had failures"
          else
            echo "has_failure=false" >> $GITHUB_OUTPUT
            echo "Pipeline completed successfully"
          fi
          
          # Capture CSV filename from pipeline output
          CSV_PATH="${{ needs.run-pipeline.outputs.csv_filename }}"
          echo "csv_path=$CSV_PATH" >> $GITHUB_OUTPUT

      - name: Run Email Notification
        env:
          PIPELINE_HAS_FAILURE: ${{ steps.status.outputs.has_failure }}
        run: |
          CMD="python3 scripts/email_notification.py --mode daily"
          
          # Pass the CSV path if available
          CSV_PATH="${{ steps.status.outputs.csv_path }}"
          if [ -n "$CSV_PATH" ] && [ -f "$CSV_PATH" ]; then
            CMD="$CMD --csv-path '$CSV_PATH'"
            echo "Using CSV file: $CSV_PATH"
          fi
          
          echo "Executing: $CMD"
          eval $CMD

  # =============================================================================
  # Job 5: Commit Results - Commit CSV files only (no logs)
  # (Runs in parallel with email-notification on GitHub-hosted runner)
  # =============================================================================
  commit-results:
    runs-on: ubuntu-latest
    environment: Production
    needs: [setup, run-pipeline]
    if: ${{ !cancelled() }}
    permissions:
      contents: write
    env:
      TZ: Asia/Singapore
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ssh-key: ${{ secrets.DEPLOY_KEY }}

      - name: Download encrypted reports
        uses: actions/download-artifact@v4
        with:
          name: pipeline-reports-encrypted
          path: encrypted_artifacts
        continue-on-error: true

      - name: Decrypt reports
        env:
          ARTIFACT_KEY: ${{ secrets.ARTIFACT_KEY }}
        run: |
          if [ -f encrypted_artifacts/reports.tar.gz.enc ]; then
            openssl enc -aes-256-cbc -d -pbkdf2 -iter 100000 \
              -in encrypted_artifacts/reports.tar.gz.enc \
              -pass pass:"$ARTIFACT_KEY" | tar -xzf -
            rm -rf encrypted_artifacts
            echo "✓ Reports decrypted successfully (folder structure preserved)"
            # Show what was extracted
            echo "Extracted files:"
            find reports -type f -name "*.csv" 2>/dev/null | head -20 || echo "No CSV files found"
          else
            echo "No encrypted reports found"
          fi

      - name: Commit and Push Results
        env:
          SPIDER_SUCCESS: ${{ needs.run-pipeline.outputs.spider_success }}
          UPLOADER_SUCCESS: ${{ needs.run-pipeline.outputs.uploader_success }}
          REPORTS_DIR: ${{ vars.REPORTS_DIR || 'reports' }}
          DAILY_REPORT_DIR: ${{ vars.DAILY_REPORT_DIR || 'reports/DailyReport' }}
          AD_HOC_DIR: ${{ vars.AD_HOC_DIR || 'reports/AdHoc' }}
        run: |
          set -e
          
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          CURRENT_BRANCH="${{ needs.setup.outputs.branch }}"
          
          # Setup SSH URL if needed
          REPO_URL=$(git remote get-url origin)
          if [[ "$REPO_URL" == https://* ]]; then
            REPO_SSH=$(echo "$REPO_URL" | sed 's|https://github.com/|git@github.com:|')
            git remote set-url origin "$REPO_SSH"
            echo "Updated remote URL to SSH: $REPO_SSH"
          fi
          
          # ============================================================
          # STEP 1: Stage all files IMMEDIATELY (before any git operations)
          # This ensures files are tracked and won't conflict with remote
          # ============================================================
          echo "Step 1: Staging files..."
          if [[ "$SPIDER_SUCCESS" == "true" && "$UPLOADER_SUCCESS" == "true" ]]; then
            echo "Spider and Uploader both succeeded - staging all CSV files including history files"
            git add "$REPORTS_DIR/parsed_movies_history.csv" 2>/dev/null || true
            git add "$REPORTS_DIR/pikpak_bridge_history.csv" 2>/dev/null || true
            git add "$REPORTS_DIR/proxy_bans.csv" 2>/dev/null || true
            find "$DAILY_REPORT_DIR" -name "*.csv" -exec git add {} \; 2>/dev/null || true
          else
            echo "Spider or Uploader failed/cancelled - skipping history files commit"
            find "$DAILY_REPORT_DIR" -name "*.csv" -exec git add {} \; 2>/dev/null || true
          fi
          find "$AD_HOC_DIR" -name "*.csv" -exec git add {} \; 2>/dev/null || true
          
          # Check if there are any changes to commit
          if git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          fi
          
          # Show what will be committed
          echo "Files to be committed:"
          git diff --cached --name-only
          
          # ============================================================
          # STEP 2: Commit locally BEFORE any remote interaction
          # This ensures our files are tracked in a commit
          # ============================================================
          echo "Step 2: Creating local commit..."
          git commit -m "Auto-commit: Daily pipeline results $(date +'%Y-%m-%d %H:%M:%S UTC')"
          LOCAL_COMMIT=$(git rev-parse HEAD)
          echo "Created local commit: $LOCAL_COMMIT"
          
          # ============================================================
          # STEP 3: Push with retry logic and proper conflict handling
          # ============================================================
          echo "Step 3: Pushing to remote..."
          echo "Target branch: $CURRENT_BRANCH"
          
          MAX_RETRIES=5
          RETRY_DELAY=3
          
          for i in $(seq 1 $MAX_RETRIES); do
            echo ""
            echo "=== Push attempt $i of $MAX_RETRIES ==="
            
            # Fetch latest from remote
            if ! git fetch origin "$CURRENT_BRANCH"; then
              echo "Fetch failed, will retry..."
              sleep $RETRY_DELAY
              RETRY_DELAY=$((RETRY_DELAY * 2))
              continue
            fi
            
            # Check if we need to integrate remote changes
            LOCAL_HEAD=$(git rev-parse HEAD)
            REMOTE_HEAD=$(git rev-parse "origin/$CURRENT_BRANCH" 2>/dev/null || echo "")
            
            if [[ -z "$REMOTE_HEAD" ]]; then
              echo "Could not determine remote HEAD, pushing directly..."
            elif [[ "$LOCAL_HEAD" == "$REMOTE_HEAD" ]]; then
              echo "Already up to date with remote"
            else
              # Check if we're ahead or behind
              MERGE_BASE=$(git merge-base HEAD "origin/$CURRENT_BRANCH" 2>/dev/null || echo "")
              
              if [[ "$MERGE_BASE" == "$REMOTE_HEAD" ]]; then
                echo "Local is ahead of remote, can push directly"
              elif [[ -n "$MERGE_BASE" ]]; then
                echo "Local is behind remote, need to rebase..."
                
                # Try rebase first (cleaner history)
                if git rebase "origin/$CURRENT_BRANCH"; then
                  echo "Rebase successful"
                else
                  echo "Rebase had conflicts, aborting and trying merge..."
                  git rebase --abort 2>/dev/null || true
                  
                  # Try merge instead
                  if git merge "origin/$CURRENT_BRANCH" --no-edit -m "Merge remote changes"; then
                    echo "Merge successful"
                  else
                    echo "Merge also failed"
                    git merge --abort 2>/dev/null || true
                    
                    # Try to accept our changes to resolve conflict
                    echo "Attempting to resolve conflicts by keeping local changes..."
                    if git rebase "origin/$CURRENT_BRANCH" -X ours 2>/dev/null; then
                      echo "Rebase with ours strategy successful"
                    else
                      git rebase --abort 2>/dev/null || true
                      echo "Could not resolve conflicts, will retry..."
                      
                      if [ $i -lt $MAX_RETRIES ]; then
                        sleep $RETRY_DELAY
                        RETRY_DELAY=$((RETRY_DELAY * 2))
                      fi
                      continue
                    fi
                  fi
                fi
              else
                echo "No common ancestor found, pushing directly..."
              fi
            fi
            
            # Try to push
            if git push origin "$CURRENT_BRANCH"; then
              echo ""
              echo "✓ Changes committed and pushed to $CURRENT_BRANCH branch successfully"
              exit 0
            else
              echo "Push failed"
              
              if [ $i -lt $MAX_RETRIES ]; then
                echo "Waiting ${RETRY_DELAY}s before retry..."
                sleep $RETRY_DELAY
                RETRY_DELAY=$((RETRY_DELAY * 2))
              fi
            fi
          done
          
          echo ""
          echo "::warning::Failed to push after $MAX_RETRIES attempts."
          exit 1
